{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_k80_gpu_001.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/DaisukeHirata/colab-in-ukraine/blob/master/test_k80_gpu_001.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gGLZBQ876SXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GPUの割り当て"
      ]
    },
    {
      "metadata": {
        "id": "9A53nrUk554N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1567b69d-3f63-423f-8c3f-096a103dc000"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "lsQ7hyHs6kgj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GPU/CPUのパフォーマンス比較"
      ]
    },
    {
      "metadata": {
        "id": "fym2uh_CxUYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b43dc12-6b71-4de3-8fc8-94bb8476c136"
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.11.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wv8PLYE6xUz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c314aec7-3e03-4aa7-84e5-dfd747bfcb0c"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.6 (default, Sep 12 2018, 18:26:19) \\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "zT816kG76c16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7a6cc91e-3446-42ce-801e-07f36e59bf69"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "\n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "10.010858926000083\n",
            "GPU (s):\n",
            "0.7928145609998865\n",
            "GPU speedup over CPU: 12x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6FGjTYuS67EF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras MNIST daetasetをインポート"
      ]
    },
    {
      "metadata": {
        "id": "0citwR7G6qkf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1zOJEdI7CUr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_CqFc1Zyh3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38b7b08e-dd7e-477b-9576-4f05efc92060"
      },
      "cell_type": "code",
      "source": [
        "keras.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "-5kzNbLp7FPn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTcO-WG07KQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "b73a9d24-567e-439f-9733-f19b3937d944"
      },
      "cell_type": "code",
      "source": [
        "mnist.load_data()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         ...,\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
              "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
              " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         ...,\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
              "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "K8joMMyb7RtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "916cd067-711b-4933-f637-6571c0fe4631"
      },
      "cell_type": "code",
      "source": [
        "'''Trains a simple convnet on the MNIST dataset.\n",
        "\n",
        "Gets to 99.25% test accuracy after 12 epochs\n",
        "(there is still a lot of margin for parameter tuning).\n",
        "16 seconds per epoch on a GRID K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.2717 - acc: 0.9174 - val_loss: 0.0555 - val_acc: 0.9828\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0902 - acc: 0.9735 - val_loss: 0.0420 - val_acc: 0.9857\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0679 - acc: 0.9796 - val_loss: 0.0327 - val_acc: 0.9890\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0547 - acc: 0.9841 - val_loss: 0.0291 - val_acc: 0.9903\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0479 - acc: 0.9858 - val_loss: 0.0315 - val_acc: 0.9898\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0436 - acc: 0.9870 - val_loss: 0.0278 - val_acc: 0.9912\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.0258 - val_acc: 0.9911\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0348 - acc: 0.9898 - val_loss: 0.0256 - val_acc: 0.9918\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0320 - acc: 0.9901 - val_loss: 0.0260 - val_acc: 0.9920\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0296 - acc: 0.9911 - val_loss: 0.0281 - val_acc: 0.9907\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0290 - acc: 0.9911 - val_loss: 0.0249 - val_acc: 0.9919\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0259 - acc: 0.9919 - val_loss: 0.0309 - val_acc: 0.9906\n",
            "Test loss: 0.030875471595154795\n",
            "Test accuracy: 0.9906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JKxW2g4Y2ABc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperdashでモニタする"
      ]
    },
    {
      "metadata": {
        "id": "iBq1OU3M2DOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "64977840-0d91-435a-e814-3f6ad8e749dc"
      },
      "cell_type": "code",
      "source": [
        "!pip install hyperdash"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperdash\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/a1/2606aa8a8c3bf083cb305dba3164cb00285f97db34080d63c22b6c413175/hyperdash-0.15.3.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hyperdash) (2.18.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from hyperdash) (1.11.0)\n",
            "Collecting python-slugify (from hyperdash)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2018.8.24)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (1.22)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->hyperdash)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\r\u001b[K    4% |█▍                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K    8% |██▉                             | 20kB 21.6MB/s eta 0:00:01\r\u001b[K    13% |████▏                           | 30kB 24.3MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 40kB 3.3MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 51kB 4.0MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 61kB 4.7MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 71kB 5.4MB/s eta 0:00:01\r\u001b[K    34% |███████████▏                    | 81kB 6.0MB/s eta 0:00:01\r\u001b[K    39% |████████████▌                   | 92kB 6.6MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 102kB 7.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 112kB 4.2MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 122kB 4.3MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 133kB 4.3MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▌            | 143kB 7.8MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 153kB 7.9MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 163kB 7.9MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 174kB 7.9MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 184kB 7.9MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 194kB 8.0MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▉    | 204kB 7.8MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 215kB 34.4MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 225kB 34.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 235kB 22.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hyperdash, python-slugify\n",
            "  Running setup.py bdist_wheel for hyperdash ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/62/5f/af/bbcaeb6570e4904c14fb4c1b70fee559a3788182ce4d104ce7\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built hyperdash python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, hyperdash\n",
            "Successfully installed Unidecode-1.0.22 hyperdash-0.15.3 python-slugify-1.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzdnwIvb2G8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}